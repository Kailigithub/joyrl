# Basic Concept

This part mainly introduces the basic principles and framework of `JoyRL`.

## Review of Concepts

In $\text{RL}$ (reinforcement learning), the agent interacts with the environment to generate samples, and constantly updates its policy to maximize the reward, as shown in Figure 1.

<div align=center>
<img width="400" src="../figs/interaction_mdp.png"/>
<div align=center>Figure 1 The process of interaction between the agent and the environment</div>
</div>

Taking the `DQN` algorithm as an example, as shown in Figure 2, in the process of interacting with the environment, the agent will continuously generate samples, that is, `transition`, mainly including `state`, `action`, `reward`, `next_state`, etc., and then store these samples in the experience pool (experience replay buffer). Then a batch of samples is randomly sampled from the experience pool for algorithm update. After the update, a batch of samples is generated, and so on, until the termination condition is reached. Almost all $\text{RL}$ algorithms follow this idea, which is the basic training logic of $\text{RL}$.

<div align=center>
<img width="500" src="../figs/DQN_pseu.png"/>
<div align=center>Figure 2 Pseudocode of DQN algorithm training</div>
</div>

At present, most of the mainstream $\text{DRL}$ (deep reinforcement learning) algorithms update the policy by updating the parameters of the neural network, that is, the model. From this point of view, it is very similar to $\text{DL}$ (deep learning). Therefore, many problems in deep learning may also be encountered in $\text{RL}$, such as overfitting, gradient explosion and disappearance, falling into local optimal solutions, etc. However, unlike $\text{DL}$, the samples used to update the model in $\text{RL}$ need to be generated by interacting with the environment continuously, rather than being prepared all at once like $\text{DL}$ . Although some offline $\text{RL}$ is studying how to avoid interacting with the environment as much as possible, most of the current $\text{RL}$ algorithms are still online, which is also an important difference between $\text{RL}$ and deep learning.

Actually, the training process of $\text{RL}$ is often more complicated, and many factors need to be considered, such as how to interact with the environment, how to store samples, how to update the policy, how to set the termination condition, etc. These factors will affect the training effect, so it is necessary to manage these factors uniformly, which is also the original intention of the design of `JoyRL`. At the same time, the sample data flow in $\text{RL}$ is also more complicated. For example, the `DQN` algorithm first stores the samples in the experience replay, and then randomly samples a batch of samples from the experience replay for update. Such data flow is already more complicated, and for some complex algorithms, such as `HER`, `PER`, `PPO`, etc., the data flow will be more complicated. In order to allow users to customize some sample processing methods according to their own needs, `JoyRL` also provides corresponding interfaces, which will be introduced in detail in the following sections.

## Interactor and Learner

As mentioned earlier, the training process of $\text{RL}$ mainly includes two processes: interaction sampling and policy update. Therefore, these two processes are abstracted into two modules in `JoyRL`, namely interactor and learner. As shown in Figure 3, the interactor obtains the updated policy from the learner each time, and then interacts with the environment to generate samples (experiences). The learner continuously obtains samples from the interactor, and then updates the algorithm. Among them, the interactor is also called `worker` in some materials, which is unified as interactor and learner here to keep consistent with the naming in `JoyRL`.

<div align=center>
<img width="600" src="../figs/interactor_learner.png"/>
<div align=center>Figure 3 Interactor and Learner</div>
</div>

Comparing Figure 1 and Figure 3, it can be seen that in order to facilitate actual training, the agent in Figure 1 is split into the interactor and learner in Figure 3 in `JoyRL`. This is because in actual training, interaction sampling and policy update can be parallelized, which can improve training efficiency. After splitting into two modules, it is also convenient for users to customize some sample processing methods.

## Collector and Model Manager

The data flow in Figure 3 is basically one-way, that is, the updated policy only flows from the learner to the interactor, and the interaction samples only flow from the interactor to the learner. However, in the actual training process, some data needs to flow in both directions, such as the `reward`, `done`, etc. in the interaction samples of the `PER` algorithm need to be passed back to the learner to update the `priority` in the experience replay, so as to ensure the normal operation of the `PER` algorithm. In addition, in the `PPO` algorithm, in the process of policy update, the `log_prob` needs to be passed back to the learner to calculate the `ratio`. However, the interactor and learner have their own main tasks, and only in these two modules can data be collected and processed in parallel, which will reduce the training efficiency. Therefore, we define a collector module to manage the collection and processing of interaction samples and policy samples, which plays the role of a data intermediate station. In addition, since updating the model, that is, backpropagation of the neural network, often takes up a lot of time, we define a model manager module to manage the saving and loading of the model, so that frequent saving and loading of the model can be avoided during the training process, thereby further improving the training efficiency.

As shown in Figure 4, the model manager continuously provides the updated model to the interactor, and also continuously obtains the updated model from the learner, and then saves it to the local. The collector continuously obtains interaction samples and policy samples from the interactor, and then stores the interaction samples in the experience replay, and passes the policy samples back to the learner to update the `priority` in the experience replay. In this way, the interactor and learner can focus on their main tasks, and avoid frequent data collection and processing, thereby improving training efficiency.

<div align=center>
<img width="600" src="../figs/collector.png"/>
<div align=center>Figure 4 Collector and Model Manager</div>
</div>

## Tracker and Recorder

In complex training, in order to improve efficiency, the number of interactors and learners must be increased. Some information such as the current round number and the number of updates need to be shared globally. Therefore, we define a tracker module to track these information. In order to facilitate users to monitor the training process, we define a recorder module to record the reward curve, loss curve, etc. generated during the training process, and write these information to the `tensorboard` file, so that users can monitor it.

As shown in Figure 5, the tracker is shared by all modules, so it is not drawn in the figure. The recorder mainly obtains reward and loss information from the interactor and learner, and then records it to the local file.

<div align=center>
<img width="600" src="../figs/recorder.png"/>
<div align=center>Figure 5 Tracker and Recorder</div>
</div>

## Online Tester

Since the training process of $\text{RL}$ is often complex and unstable, we often need to test the policy regularly during the training process in order to find problems in time. Therefore, we define an online tester module to test the performance of the policy online. The online tester regularly obtains the updated model from the model manager, and then tests the performance of the policy in the test environment. After the test, the test results will be recorded in the recorder.

As shown in Figure 6, the online tester is more like an independent module, which does not affect the training process, so it is specially separated in the figure.

<div align=center>
<img width="600" src="../figs/recorder.png"/>
<div align=center>Figure 6 Overall framework</div>
</div>

Up to now, the overall framework of `JoyRL` has been introduced. Interested readers can refer to the source code of `JoyRL`.

## Directory Tree

The directory tree of `JoyRL` is as follows:

```python
|-- joyrl
    |-- algos # algorithm folder
        |-- [Algorithm name] # refers to the algorithm name such as DQN
            |-- config.py # store AlgoConfig
                |-- class AlgoConfig # class for algorithm parameter settings
            |-- policy.py # store policy
            |-- data_handler.py # store data handler
    |-- framework # framework folder, store some modules, etc.
        |-- envs # store built-in environments
            |-- gym # store gym environment
                |-- config.py # store default parameter settings for gym environment
                    |-- class EnvConfig # class for environment parameter settings
        |-- config.py # store common parameter settings
            |-- class GeneralConfig # class for common parameter settings
|-- presets # preset parameters, corresponding results are stored in benchmarks
|-- benchmarks # store trained results
|-- docs # documentation directory
|-- tasks # will be generated automatically during training
|-- README.md # project README
|-- requirements.txt # Pyhton dependency list
```
